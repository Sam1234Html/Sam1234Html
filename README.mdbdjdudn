# -*- coding: utf-8 -*-
"""
AI Tools Assignment - Part 2, Task 1: Classical ML with Scikit-learn
Dataset: Iris Species Dataset
Goal: Preprocess, train Decision Tree, evaluate.
"""

# Import necessary libraries
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report
import numpy as np

print("--- Task 1: Classical ML with Scikit-learn ---")

# 1. Load the Iris Species Dataset
# The Iris dataset is well-known and often used for classification examples.
# It's clean and doesn't typically require extensive missing value handling.
iris = load_iris()
X = iris.data
y = iris.target
feature_names = iris.feature_names
target_names = iris.target_names

print("\nDataset loaded successfully.")
print(f"Features (X) shape: {X.shape}")
print(f"Target (y) shape: {y.shape}")
print(f"Feature names: {feature_names}")
print(f"Target names: {target_names}")

# Convert to DataFrame for easier inspection (optional, but good practice)
df = pd.DataFrame(X, columns=feature_names)
df['species'] = y
print("\nFirst 5 rows of the dataset:")
print(df.head())

# 2. Preprocess the data (handle missing values, encode labels)
# The Iris dataset is clean; no missing values to handle here.
# Target labels (y) are already encoded as integers (0, 1, 2), which is suitable for Scikit-learn.
print("\nData preprocessing step:")
if np.isnan(X).sum() == 0:
    print("No missing values found in the dataset.")
else:
    print("Missing values found and handled (e.g., imputation or removal).") # Placeholder if dataset needed handling

# 3. Split the data into training and testing sets
# A common split is 80% for training and 20% for testing.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
# stratify=y ensures that the proportion of target labels is the same in train and test sets.

print(f"\nData split into training and testing sets:")
print(f"X_train shape: {X_train.shape}")
print(f"X_test shape: {X_test.shape}")
print(f"y_train shape: {y_train.shape}")
print(f"y_test shape: {y_test.shape}")


# 4. Train a Decision Tree Classifier
print("\nTraining Decision Tree Classifier...")
decision_tree_model = DecisionTreeClassifier(random_state=42)
decision_tree_model.fit(X_train, y_train)
print("Decision Tree Classifier trained.")

# 5. Make predictions on the test set
y_pred = decision_tree_model.predict(X_test)

# 6. Evaluate the model using accuracy, precision, and recall
print("\nModel Evaluation:")

# Accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Precision, Recall, and F1-Score for each class, and overall averages.
# 'weighted' average is suitable for imbalanced datasets, but 'macro' or 'micro' could also be used.
# For multi-class classification, `precision_score` and `recall_score` need `average` parameter.
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')

print(f"Precision (weighted): {precision:.4f}")
print(f"Recall (weighted): {recall:.4f}")

# Detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred, target_names=target_names))

print("\n--- Task 1 Complete ---")

