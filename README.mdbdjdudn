# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# --- 1. Choose Your SDG and Problem ---
# SDG: SDG 13 - Climate Action
# Specific Problem: Forecasting carbon emissions to support policy-making and emission reduction strategies.

# --- 2. Select an ML Approach ---
# Approach: Supervised Learning (Regression)
# We will predict a continuous numerical value (CO2 emissions) based on historical features.

# --- 3. Dataset & Tools ---
# Dataset: For demonstration purposes, we will create a synthetic dataset.
# In a real-world scenario, you would use actual datasets like:
# - World Bank Open Data (e.g., CO2 emissions, GDP per capita, population growth, energy use)
# - UN SDG Database
# - Kaggle datasets related to climate change or energy statistics.

# Tools: Python, Pandas (for data manipulation), Scikit-learn (for ML model), Matplotlib/Seaborn (for visualization).

# --- Synthetic Data Generation ---
# Let's assume we have data on 'Year', 'GDP_per_capita', 'Population_growth', 'Energy_consumption_per_capita',
# and 'CO2_emissions_metric_tons_per_capita'.
np.random.seed(42) # for reproducibility

years = np.arange(1990, 2024)
num_samples = len(years)

# Simulate features with some trends and noise
gdp_per_capita = np.linspace(5000, 30000, num_samples) + np.random.normal(0, 2000, num_samples)
population_growth = np.linspace(0.01, 0.005, num_samples) + np.random.normal(0, 0.001, num_samples)
energy_consumption_per_capita = np.linspace(2000, 8000, num_samples) + np.random.normal(0, 500, num_samples)

# Simulate CO2 emissions with a dependency on other features and a general upward trend
co2_emissions = (
    0.5 * gdp_per_capita / 1000  # GDP influence
    + 500 * energy_consumption_per_capita / 1000 # Energy consumption influence
    + 1000 * (years - 1990) / 30 # Time trend
    + np.random.normal(0, 500, num_samples) # Noise
)
co2_emissions = np.maximum(0, co2_emissions) # Ensure no negative emissions

data = pd.DataFrame({
    'Year': years,
    'GDP_per_capita': gdp_per_capita,
    'Population_growth': population_growth,
    'Energy_consumption_per_capita': energy_consumption_per_capita,
    'CO2_emissions_metric_tons_per_capita': co2_emissions
})

print("--- Simulated Dataset Head ---")
print(data.head())
print("\n--- Dataset Info ---")
print(data.info())
print("\n--- Dataset Description ---")
print(data.describe())

# --- 4. Build Your Model ---

# Preprocess Data:
# For this simple dataset, cleaning involves checking for NaNs (none in synthetic data).
# Normalization/Scaling might be beneficial for more complex models, but for Linear Regression, it's less critical.
# We will use 'Year', 'GDP_per_capita', 'Population_growth', 'Energy_consumption_per_capita' as features (X)
# and 'CO2_emissions_metric_tons_per_capita' as the target (y).

X = data[['Year', 'GDP_per_capita', 'Population_growth', 'Energy_consumption_per_capita']]
y = data['CO2_emissions_metric_tons_per_capita']

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"\nTraining set shape: {X_train.shape}, {y_train.shape}")
print(f"Testing set shape: {X_test.shape}, {y_test.shape}")

# Train Model:
model = LinearRegression()
model.fit(X_train, y_train)

print("\n--- Model Training Complete ---")
print(f"Model Coefficients: {model.coef_}")
print(f"Model Intercept: {model.intercept_}")

# Evaluate:
y_pred = model.predict(X_test)

mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print(f"\n--- Model Evaluation Metrics ---")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")
print(f"R-squared (R2): {r2:.2f}")

# Visualize Results
plt.figure(figsize=(12, 6))
plt.scatter(y_test, y_pred, alpha=0.7)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect Prediction')
plt.xlabel("Actual CO2 Emissions")
plt.ylabel("Predicted CO2 Emissions")
plt.title("Actual vs. Predicted CO2 Emissions")
plt.legend()
plt.grid(True)
plt.show()

# Residuals Plot (to check for patterns in errors)
residuals = y_test - y_pred
plt.figure(figsize=(12, 6))
sns.histplot(residuals, kde=True)
plt.title("Distribution of Residuals")
plt.xlabel("Residuals (Actual - Predicted)")
plt.ylabel("Frequency")
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 6))
plt.scatter(y_pred, residuals, alpha=0.7)
plt.axhline(y=0, color='r', linestyle='--')
plt.xlabel("Predicted CO2 Emissions")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values")
plt.grid(True)
plt.show()

# --- 5. Ethical Reflection ---

# How might bias in your data affect outcomes?
# 1. Historical Data Bias: If the historical data for CO2 emissions or related features
#    (like GDP, energy consumption) is skewed towards certain regions, countries, or
#    economic periods, the model might not generalize well to underrepresented areas
#    or future scenarios that differ significantly from the past.
# 2. Feature Selection Bias: If crucial features influencing CO2 emissions (e.g.,
#    policy changes, technological advancements, natural disasters) are not included,
#    the model's predictions could be inaccurate or misleading.
# 3. Data Collection Inconsistencies: Variations in how data was collected across
#    different sources or over time can introduce noise and bias.
# Consequences of Bias:
# - Policies based on biased forecasts might unfairly burden certain regions or
#   industries, or fail to address the true drivers of emissions in others.
# - Over- or under-estimation of emissions could lead to ineffective climate actions.

# How does your solution promote fairness and sustainability?
# 1. Informed Policy Making: By providing a forecast of CO2 emissions, the model helps
#    governments and organizations understand potential future trends. This allows for
#    proactive development of policies and interventions (e.g., carbon taxes,
#    renewable energy incentives) to mitigate climate change.
# 2. Resource Allocation: Accurate forecasts can help in allocating resources more
#    effectively towards sustainable initiatives, such as investment in green
#    technologies or climate resilience projects.
# 3. Accountability: The model can serve as a tool for monitoring the effectiveness
#    of current climate policies and holding stakeholders accountable for their
#    emission reduction targets.
# 4. Awareness and Education: Visualizing emission trends can raise public awareness
#    about climate change and encourage sustainable behaviors.
# 5. Iterative Improvement: The model can be continuously updated with new data and
#    refined to improve its accuracy, adapting to changing global dynamics and
#    technological advancements.

# --- Example of making a prediction for a future year (Stretch Goal) ---
# Assuming 2024 as the last year in our training data, let's predict for 2025.
# We'd need to estimate the input features for 2025. This highlights the dependency on future data.
# For simplicity, let's extrapolate based on the last year's trends from our synthetic data.

last_year_data = data.iloc[-1]
future_year = last_year_data['Year'] + 1 # Predict for 2024 based on 2023 data

# For a real prediction, these values would come from economic forecasts, population projections etc.
# Here, we'll just slightly increment based on observed trends or make a reasonable guess.
future_gdp_per_capita = last_year_data['GDP_per_capita'] * 1.02 # Assuming 2% growth
future_population_growth = last_year_data['Population_growth'] * 0.98 # Assuming slight decline
future_energy_consumption_per_capita = last_year_data['Energy_consumption_per_capita'] * 1.01 # Assuming 1% growth

future_data = pd.DataFrame([[
    future_year,
    future_gdp_per_capita,
    future_population_growth,
    future_energy_consumption_per_capita
]], columns=X.columns)

predicted_co2_2025 = model.predict(future_data)

print(f"\n--- Future Prediction (Example) ---")
print(f"Predicted CO2 Emissions for {int(future_year)}: {predicted_co2_2025[0]:.2f} metric tons per capita")

# Note on Real-time Data / Deployment:
# For real-time data, you would replace the synthetic data generation with API calls
# to relevant data sources (e.g., World Bank API for economic indicators).
# For deployment, frameworks like Flask or Streamlit would wrap this Python code
# into a web application where users could input parameters and get predictions.
